
\documentclass[a4paper,12pt]{article}
\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}

\title{\textbf{Trabalho Prático de Compiladores I \\ Etapa 2: Analisador Léxico}}
\author{
        Felipe Buzatti e Letícia Lana Cherchiglia \\
	\{buzatti,letslc\}@dcc.ufmg.br \\ \\
        \textit{Departamento de Ciência da Computação}\\
        \textit{Universidade Federal de Minas Gerais}\\
}
\date{09 de Abril de 2011}


\begin{document}

\maketitle

\section{Introdução}
\label{intro}

O objetivo desse trabalho é apresentar um analisador léxico para a linguagem minL. A idéia de um analisador léxico é identificar em uma seqüência de caracteres de um arquivo de texto determinados padrões a que chamamos de ``tokens''.
Muitas vezes ao se produzir um analisador léxico utilizamos uma ferramenta, capaz de produzir código para o analisador a partir de uma notação de expressões regulares para identificar os tokens. No entanto, nesse presente trabalho procurou-se criar um analisador sem o auxílio de uma ferramenta desse tipo. Um dos motivos para tal decisão foi a possibilidadde de compreender completamente o funcionamento do analisador produzido.

Esse analisador léxico não é um programa estanque, ele deve poder trabalhar com outros módulos que venham a formar um compilador, módulos tais como uma tabela de símbolos, um analisador sintático, um otimizador de código, etc. De fato, a idéia desse trabalho é encarar esse analisador como uma peça de um compilador.

\section{Implementação}
Como dito em~\ref{intro} esse analisador foi construído sem o auxílio de ferramentas, tais como o (F)Lex. Um dos fortes motivos foi ter uma experiência didática mais rica, na medida em que é possível conviver, e até compreender os problemas enfrentados por utilitário desses.

A idéia da implementação passa pelo fato de que há alguns elementos-chave para diferenciar os tokens. Em especial podemos notar que um espaço em branco, tabulação ou quebra de linha sempre marca o início/fim de um token; podemos também notar que o mesmo pode ser válido para alguns símbolos, que também são tokens, tais como ',' , '(' , '+' e assim por diante. Há ainda uma terceira propriedade chave que permite construir um analisador léxico mais facilmente, o fato de que é possível reconhecer entre uma palavra e um número olhando apenas para o primeiro caractere, uma vez que palavras nunca começam com números.

Esses são exemplos de características que foram notadas no padrão de tokens, e permitiram criar o analisador ``na mão''.
De forma semelhante ao que uma ferramenta automatizada faria a idéia desse analisador também passa por uma noção de máquina de estados, em que cada novo caractere lido pode alterar o estado do programa, e definir qual token deverá ser definido. Isso é feito através de um ``switch'' que faz essa seleção de acordo com um tipo de caracteres, e a partir de alguns desvios condicionais, ou mais alguns estados é capaz de definir se trata-se de um caractere especial (tais como ',' ou ')') ou de uma palavra, ou de um número.

Depois de definir esses grupos mais gerais é preciso delimitar dentro deles, pois embora o ``switch'' seja capaz de definir o caractere especial, dizer se trata-se de um identificador ou palavra reservada, ou entre um inteiro e um número real exige que se analise um número maior de estados. 

Para isso usa-se duas estratégias. No caso de definir se um número é real ou inteiro a idéia é procurar por símbolos caracteres exclusivos dos números reais, tais como '.' e 'E'. Para as palavras reservadas a idéia é manter uma base com a qual toda palavra encontrada é comparada. Essa sugestão pode ser encontrada no livro~\cite{dragao}. Para os casos em que há um casamento da palavra com a base basta usar as informações da base. Quando a palavra não está na base podemos assumir que se trata de um identificador, então exportamos essa informação.

\section{Formato dos dados}
\subsection{Entrada}
A entrada do programa pode ser feita em qualquer arquivo de texto puro. Obviamente a execução correta pressupõe que esse arquivo de texto contenha programas válidos da linguagem minL, mas o analisador é capaz de capturar erros para situações inesperadas, abortando a análise.

\subsection{Saída}
O objetivo desse analisador léxico, como mencionado anteriormente é ser capaz de transformar arquivos de texto contendo programas da linguagem minL em tokens que venham a ser posteriormente enviados para outras etapas do compilador, tais como o analisador sintático.

Esse objetivo significa que não há aqui uma grande preocupação com uma saída mais trabalhada desse programa, visando apenas a uma forma de testar o seu funcionamento prático. Isso nos leva, logicamente, a uma mera impressão dos tokens à medida que eles vão sendo reconhecidos.

\section{Uso do analisador}
O programa analisador deve ser compilado usando o \textit{gcc}.
Seu uso como um programa separado tem um caráter necessariamente de teste. A sintaxe nesse caso é simplesmente:
\begin{verbatim}
 <nome_do_programa> <nome_do_arquivo_minL>
\end{verbatim}
A saída nesse caso consiste em duas colunas, e um token por linha.

Para um uso mais real ele deve ser acoplado a outras estruturas do compilador. Para fazê-lo é preciso chamar a função \textit{analise$\_$lex$()$}, cujo protótipo é:
\begin{verbatim}
 int analise_lex(FILE * arquivo);
\end{verbatim}
Essa função retorna após ler uma linha inteira do arquivo, indicando através do valor de retorno se a análise terminou apenas uma linha ou se já atingiu o final do arquivo. 
Para fazer uso dos tokens produzidos pelo analisador basta modificar as funções
\begin{verbatim}
 void exporta_buffer(char buffer[TAM_BUFFER_ENTR], 
						  int * ind_buffer, flag tipo);
 void exporta_simbolo(char valor, char tipo_simb[TAM_TIPO_SIMB]);
 void exporta_buffer_esp(char valor[TAM_BUFFER_ESP], 
						  char tipo_simb[TAM_TIPO_SIMB]);
\end{verbatim}
que no momento apenas imprime tais tokens, como já foi dito.

\section{arquivos utilizados}
A implementação está separada em um arquivo de headers, e um arquivo '.c'. Uma cópia impressa de ambos arquivos acompanha essa documentação, anexa ao final.

\section{Testes}
Para testar a corretude do analisador foram feitos testes usando seis exemplos contidos na especificação do trabalho, que se encontram em anexo, após o código. Também se encontram em anexo os respectivos resultados destes testes.

\section{Conclusão}
O analisador léxico foi construído sem o uso de ferramentas auxiliares, e funciona corretamente. O benefício didático parece ter sido bastante grande.
Ainda que as ferramentas sejam usadas na prática por muitas pessoas/empresas, a implementação do programa totalmente por mãos humanas pode tornar o processo de correção de erros mais ágil e econômico.
\bibliography{Anal_Lex}
\bibliographystyle{sbc}
\end{document}

